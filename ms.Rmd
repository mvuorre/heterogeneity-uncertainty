---
title: 'Bayesian assessment of heterogeneity in psychological phenomena'
shorttitle: Heterogeneity in psychology
leftheader: Heterogeneity in psychology
author: 
  - name: Matti Vuorre
    affiliation: 1
    corresponding: yes
    address: Tilburg School of Social and Behavioral Sciences, Tilburg University
    email: m.j.vuorre@tilburguniversity.edu
  - name: Matthew Kay
    affiliation: 2
  - name: Niall Bolger
    affiliation: 3
affiliation:
  - id: 1
    institution: Department of Social Psychology, Tilburg University
  - id: 2
    institution: Computer Science and Communication Studies, Northwestern University
  - id: 3
    institution: Department of Psychology, Columbia University
abstract: |
  Advances in statistics and data collection methods have brought variability in psychological phenomena to the forefront of theoretical development. Yet, established methods and practices for calculating and describing heterogeneity are lacking in how they treat uncertainty regarding heterogeneity in the phenomenon under study. In the current work, we aim to deliver methods and guidelines for more complete treatments of heterogeneity in psychological phenomena.
  We provide a walkthrough of probabilistic methods for i. estimating, ii. summarising, and iii. describing heterogeneity. The methods we propose---bayesian hierarchical models---are especially suitable for investigating heterogeneity because they naturally describe uncertainty in all model parameters.
  We hope that the proposed methods help researchers to move from reporting point estimates to drawing inferences about magnitudes and differences in heterogeneity in psychological phenomena across different populations.
keywords: variation, heterogeneity, hierarchical models, mixed models, methodology
authornote: |
  \noindent \textbf{Working paper. Proceed with caution.}
wordcount: "`r wordcountaddin:::word_count(filename = 'ms.Rmd')`"
bibliography: references.bib
floatsintext: yes
numbersections: false
linenumbers: no
draft: no
mask: no
figurelist: no
tablelist: no
footnotelist: no
documentclass: apa7
classoption: jou
header-includes:
  - |
    \usepackage{float}
    \setlength{\parskip}{4pt}
output: 
  papaja::apa6_pdf:
    number_sections: true
    keep_tex: false
    highlight: kate
  papaja::apa6_docx: 
    number_sections: true
    keep_tex: false
---

```{r setup, include = FALSE}
# Packages
library(papaja)
library(janitor)
library(scales)
library(ellipse)
library(knitr)
library(latex2exp)
library(lme4)
library(ggpp)
library(patchwork)
library(latex2exp)
library(ggdist)
library(ggstance)
library(tidybayes)
library(distributional)
library(parameters)
library(brms)
library(tidyverse)

# Output options
opts_chunk$set(
  eval = TRUE,
  cache = TRUE,
  warning = FALSE,
  error = TRUE,
  message = FALSE
)

# Plotting options
theme_set(
  theme_classic(base_size = 9) +
    theme(
      strip.text = element_text(color = "black", hjust = 0),
      panel.grid = element_blank(),
      line = element_line(linewidth = .25)
    )
)

# Model estimation options
dir.create("models", FALSE)
options(
  mc.cores = 4,
  brms.backend = ifelse(require(cmdstanr), "cmdstanr", "rstan")
)

```

```{r data, include = FALSE}
# Load, save, & clean example data from Bolger et al. (2019)
dir.create("data", FALSE)
path <- "data/bolger-etal.zip"
if (!file.exists(path)) {
  download.file(
    "https://github.com/kzee/heterogeneityproject/archive/refs/heads/master.zip",
    destfile = path
  )
  unzip(path, exdir = dirname(path), junkpaths = TRUE)
}

# Save variables as numerical & factor (might need for more complex contrast/eff/etc codes)
dat <- read_csv("data/heterogeneity_dataset1_traitvalence.csv") %>% 
  # Cleaning as in Bolger et al 2019
  filter(
    response.keys == "up",
    scale(rt) < 3,
    !(id %in% c(250, 257, 272))
  ) %>% 
  # Some more cleaning
  mutate(
    person = fct_anon(factor(id)),
    trial,
    valence = factor(valenceE, levels = c(-0.5, 0.5), labels = c("Negative", "Positive")),
    rt = logrt
  ) %>% 
  select(person, trial, valence, rt) %>% 
  arrange(person, trial)
```

```{r contrast-coding, include = FALSE}
contrasts(dat$valence) <- c(-0.5, 0.5)
contrasts(dat$valence)
```

# Introduction

When building and testing theories of psychological phenomena, psychologists have long focused on asking whether an effect exists or not. Yet, establishing that an independent variable affects a dependent variable on average may not be a sufficient description of the phenomenon if the treatment effect is highly variable from one individual to another. The importance of such between-person variation, or *heterogeneity*, for theory development is widely recognized, yet rarely communicated sufficiently in the empirical literature [@bolgerCausalProcessesPsychology2019; @brandCausalEffectHeterogeneity2013]. 

One reason for the scarcity of reporting and interpreting heterogeneity is that psychologists still tend to analyze data with models that cannot address between-person variability, such as ANOVA [@bolgerCausalProcessesPsychology2019]. Although more appropriate mixed-effects (or multilevel, hierarchical) models are rapidly becoming more widespread, we believe a second important reason is that researchers do not yet have the conceptual and practical tools required to estimate and describe heterogeneity to a sufficiently informative degree. In the current work, we propose several practical methods, descriptions, and visualizations that allow researchers to better estimate, interpret, and report heterogeneity in a manner that maximizes empirical results' impact on theory building.

## Current work

Our aim is to outline methods for studying and communicating heterogeneity in a manner that fully takes uncertainty into account. We first introduce an example dataset from social psychology and review established procedures for estimating and communicating the expected heterogeneity in causal effects in the population. We then outline useful summaries of heterogeneity that focus on informative numerical and graphical descriptions, and note how they are limited by established methods' focus on point estimates of model parameters.

We then turn to our proposed method of interrogating and communicating causal effect heterogeneity using probabilistic methods. By estimating probability distributions over model parameters, these bayesian methods are uniquely suitable for examining heterogeneity and researchers' uncertainty about it. Moreover, advances in computational procedures and statistical software make these methods available to researchers with a minimal learning curve beyond established maximum likelihood based methods.

In the following sections, we detail calculations and graphics that make use of bayesian methods in communicating descriptions of heterogeneity such that uncertainty in the model parameters and, subsequently, heterogeneity, is taken into account. We then further extend the practice and consequent benefits of probabilistic inference for causal effect heterogeneity to comparing heterogeneity between populations and across timepoints in further example analyses.

We conclude by discussing additional benefits of probabilistic inference in psychology in general, and for multilevel models specifically. We emphasize that these benefits come with little or no cost to applied practitioners. We also discuss our findings regarding causal effect heterogeneity in light of this new perspective.

# Review of expected heterogeneity in hierarchical models

To begin our exposition, we reproduce the analyses presented in @bolgerCausalProcessesPsychology2019. In their study, which replicated findings first presented in @scholerInflatingDeflatingSelf2014, 62 participants saw twenty positively and twenty negatively valenced words, and judged whether each word was self-descriptive or not. Because most people are typically motivated to view themselves positively, @bolgerCausalProcessesPsychology2019 predicted that response times to positively valenced words would be shorter than to negatively valenced words [@scholerInflatingDeflatingSelf2014]. 

## Model 1

```{r}
#| label: tab-dat1
dat %>% 
  head() %>% 
  apa_table(
    span_text_columns = FALSE, 
    digits = c(0,0,0,2),
    caption = "First six rows of example dataset 1.", 
    note = "rt is log-transformed."
  )
```

In this section, we replicate @bolgerCausalProcessesPsychology2019's analysis, using their openly available data (https://github.com/kzee/heterogeneityproject). We first cleaned the data as in @bolgerCausalProcessesPsychology2019, leaving `r number(nrow(dat), big.mark = ",")` trials from 59 participants that were endorsed as self-relevant. (We present all our code in the online analysis supplement.) We show a sample of these data in Table \@ref(tab:tab-dat1). Then, we estimated the same statistical model with these data (Model 1; equations 1.1-1.3). First, we model the (log) reaction time of subject $j$ on trial $i$ as a random draw from a normal distribution with mean $\eta$ (*eta*, which can differ between trials $i$ and individuals $j$) and standard deviation $\sigma$ (*sigma*):

\begin{equation}
\tag{1.1}
\text{RT}_{ij} \sim \operatorname{Normal}(\eta_{ij}, \sigma^2).
\end{equation}

The lack of subscripts on $\sigma$ indicates that we assume it to be invariant across trials and subjects. Then, we specify a model of the mean $\eta_{ij}$ such that the regression coefficients capture our substantive questions: 

\begin{equation}
\tag{1.2}
\eta_{ij} = \beta_0 + \gamma_{0j} + (\beta_1 + \gamma_{1j})\text{V}_{ij}.
\end{equation}

This equation includes two kinds of parameters: $\beta_0$ (*beta*), the intercept, and $\beta_1$, the slope or effect of valence (V), do not have subscripts. That means that they are not further modelled, and therefore sometimes called "fixed". We contrast coded valence such that negative words are coded as -0.5, and positive words as 0.5. This coding results in an intercept that corresponds to the average reaction time across negative and positive words, and a slope coefficient that reflects the difference in log(rt) between negative and positive words. The second set of parameters, $\gamma_{0j}$ (*gamma*) and $\gamma_{1j}$ are subject-specific deviations from the average intercept and slope, respectively. That is, $\beta_0 + \gamma_{01}$ is the average reaction time for subject $j=1$. The $\gamma$ coefficients have subscripts, which in this context indicates that they are further modelled, and therefore sometimes called "random" [@gelmanDataAnalysisUsing2007]. As is standard in multilevel modelling, we model $\gamma_0$ and $\gamma_1$ as multivariate normal distributed:

\begin{equation}
\tag{1.3}
\begin{bmatrix} 
\gamma_{0} \\ \gamma_{1}
\end{bmatrix} \sim 
\operatorname{MVN}(
\begin{bmatrix} 0 \\ 0 \end{bmatrix}, 
\begin{bmatrix} 
\tau_{\gamma_{0}} &\rho_{\gamma_0\gamma_1} \\ 
\rho_{\gamma_0\gamma_1} &\tau_{\gamma_{1}} 
\end{bmatrix}
).
\end{equation}

In these equations, we assume that the subject-specific deviations have means of zero, standard deviations $\tau$ (*tau*), and a correlation $\rho$ (*rho*). Note that the $\gamma$s are zero-centered because the means are added to them in the above linear mixed-effects equation for $\eta_{ij}$. What these equations mean substantively is that the extent to which the effect of valence varies around the average effect ($\beta_1$) is estimated by the standard deviation $\tau_{\gamma_1}$. Moreover, $\rho$ indicates the extent to which individuals' average reaction times correlate with how much their reaction time is affected by valence. 

With data described in Table \@ref(tab:tab-dat1) loaded to R, we can estimate this model using standard maximum likelihood methods as implemented in the R package lme4 [@batesFittingLinearMixedEffects2015; @rcoreteamLanguageEnvironmentStatistical2023].

```{r fit1-lmer}
fit1_lmer <- lmer(
  rt ~ 1 + valence + (1 + valence | person),
  data = dat
)
```

```{r}
# Create a table of coefficients
tab1_lmer <- model_parameters(fit1_lmer, effects = "all") %>% 
  tibble() %>% 
  select(Parameter, Coefficient, starts_with("CI_"))

# Bootstrap CIs for variance parameters
tab1_lmer[3:6,3:4] <- confint(fit1_lmer, method = "boot", nsim = 100)[c(1,3,2,4),1:2]
```

```{r fit1-brms}
fit1 <- brm(
  rt ~ 1 + valence + (1 + valence | person),
  data = dat,
  file = "models/brm-fit-1",
  control = list(adapt_delta = .95)
)
```

```{r}
#| label: tbl-fit-1-brms
#| cache: false

tab1_brms <- parameters(fit1, centrality = "mean", effects = "all") %>% 
  tibble() %>% 
  select(Parameter, Mean, starts_with("CI_"))
```

```{r}
#| label: tbl-fit-1
#| cache: false

m1_parnames <- c(
  "$\\beta_0$", "$\\beta_1$", 
  "$\\tau_0$", "$\\tau_1$", 
  "$\\rho$", "$\\sigma$"
)

tab1_lmer %>% 
  mutate(
    across(where(is.numeric), ~number(., .01)),
    MLE = str_glue("{Coefficient} [{CI_low}, {CI_high}]"),
  ) %>% 
  select(Parameter, `Estimate [95\\%CI]` = MLE) %>% 
  mutate(
    Parameter = m1_parnames
  ) %>% 
  apa_table(
    span_text_columns = FALSE,
    caption = "Parameter estimates from Model 1.",
    escape = FALSE
  )
mu1n <- tab1_lmer[2,2,T]
mu1 <- number(mu1n, .01)
sd1n <- tab1_lmer[4,2,T]
sd1 <- number(sd1n, .01)
```

We show a traditional summary of the estimated parameters from this model in Table \@ref(tab:tbl-fit-1). The expected effect of positive valence on RT for the average person is approximately `r mu1` log seconds, and the expected standard deviation describing this effect's degree of variability in the population is approximately `r sd1`. 

## Heterogeneity distribution

```{r descriptors, include = FALSE}
# Heterogeneity interval
hi90_num <- mu1n + qnorm(c(0.05, .95)) * sd1n
hi90 <- paste(
  number(hi90_num, .01), 
  collapse = ", "
)
hi90_low <- paste(
  number(tab1_lmer[2,3,T] + qnorm(c(0.05, .95))*tab1_lmer[4,3,T], .01), 
  collapse = ", "
)

# Proportions
p_less_zero <- percent(pnorm(0, mu1n, sd1n), .1)
p_rope <- percent(pnorm(0.1, mu1n, sd1n) - pnorm(-0.1, mu1n, sd1n), .1)

# Ratio
ratio <- number(abs(sd1n / mu1n), .01)
```

```{r}
#| label: hdist-1
#| fig.height: 2.2
#| fig.width: 8
#| fig.env: "figure*"
#| fig.cap: The heterogeneity distribution and various descriptions of the expected heterogeneity in valence effects from Model 1. \textbf{A}. The normal density curve defined by the point estimates of the valence effect distribution's mean ($\beta_1$) and standard deviation ($\tau_1$). Shaded areas represent areas under the normal curve within 1 (dark) and 2 (light) standard deviations of the mean. \textbf{B}. The 90\% Heterogeneity Interval as represented by a line segment with arrows, and the dark shaded area. \textbf{C}. Proportion of negative valence effects in the population (dark). \textbf{D}. Proportion of valence effects in the population that are within the region of practical equivalence to zero (ROPE; dark).

p0 <- ggplot() +
  aes(xdist = dist_normal(mu1n, sd1n)) +
  scale_x_continuous(breaks = extended_breaks()) +
  coord_cartesian(xlim = c(-0.55, 0.25)) +
  theme(
    axis.title.y = element_blank(),
    axis.title.x = element_text(lineheight = rel(0.5))
    ) +
  scale_fill_manual(values = c("grey90", "grey80"), na.value = "white")
d1 <- dist_normal(mu1n, sd1n) %>% 
  mean_qi(.width = c(.66, .9))

p1 <- p0 +
  stat_slab(
    aes(fill = after_stat(level)), 
    .width = c(.68, .95), 
    p_limits = c(.000001, .999999), 
    slab_color = "grey10",
    slab_linewidth = .4
  ) +
  geom_text(
    label = TeX("$\\tau_1$"),
    x = mu1n + .05, y = .175,
    size = 3,
    parse = TRUE
  ) +
  annotate(
    "segment",
    x = mu1n, xend = mu1n + sd1n, y = .1, yend = .1,
    linewidth = .35,
    arrow = arrow(length = unit(4, "pt"), type = "closed")
  ) +
  geom_text(
    label = TeX("$\\beta_1$"),
    x = mu1n-.05, y = .075,
    size = 3,
    parse = TRUE
  ) +
  annotate(
    "segment",
    x = mu1n, xend = mu1n, y = .1, yend = 0,
    linewidth = .35,
    arrow = arrow(length = unit(4, "pt"), type = "closed")
  ) +
  theme(axis.title.y = element_text(angle = 90))

p2 <- p0 + 
  stat_slab(
    aes(fill = after_stat(level)), 
    .width = c(.90, 1), 
    p_limits = c(.000001, .999999), 
    slab_color = "grey10",
    slab_linewidth = .4
  ) +
  geom_text(
    label = TeX("$HI_{90}$"),
    x = mu1n, y = .175,
    size = 3,
    parse = TRUE
  ) +
  annotate(
    "segment",
    x = d1$.lower[2], xend = d1$.upper[2], y = .1, yend = .1,
    linewidth = .35,
    arrow = arrow(length = unit(4, "pt"), type = "closed", ends = "both")
  )

p3 <- p0 + 
  stat_slab(
    aes(fill = after_stat(x < 0)), 
    p_limits = c(.000001, .999999), 
    slab_color = "grey10",
    slab_linewidth = .4
  ) +
  geom_text(
    label = TeX('$p^d$'),
    x = mu1n, y = .175,
    size = 3,
    parse = TRUE
  )

p4 <- p0 + 
  stat_slab(
    aes(fill = after_stat(between(x, -.1, .1))), 
    p_limits = c(.000001, .999999), 
    slab_color = "grey10",
    slab_linewidth = .4
  ) +
  geom_text(
    label = TeX('$p^{ROPE}$'),
    x = 0, y = .07,
    size = 3,
    parse = TRUE
  )

(p1 | p2 | p3 | p4) &
  scale_y_continuous(
    expand = expansion(c(0.0, 0.05))
  ) &
  labs(
    y = "Density",
    x = "Valence effect"
  ) &
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank(),
    legend.position = "none"
  ) &
  plot_annotation(tag_levels = "A")
```

The point estimates in Table \@ref(tab:tbl-fit-1) define the expected normal distribution of valence effects in the population, visualized in Figure \@ref(fig:hdist-1)A. As such, Normal(`r mu1`, `r sd1`^2^) is a valid description of heterogeneity in valence effects. However, it is an incomplete description of heterogeneity for two reasons. First, it does not necessarily communicate the degree of heterogeneity in clear and actionable terms. Second, it ignores the uncertainty with which the heterogeneity parameters are estimated. 

### Interval descriptors

Having estimated the parameters of the heterogeneity distribution, we now turn to using them to define alternative, more informative descriptions of the degree of heterogeneity in valence effects in the population.

To begin with, and following @bolgerCausalProcessesPsychology2019, we used the point estimates in Table \@ref(tab:tbl-fit-1) to construct an expected *heterogeneity interval* that describes a range within which a certain percentage of the population's slopes are expected to fall. By convention, @bolgerCausalProcessesPsychology2019 and others have focused on the 95% heterogeneity interval ($HI_{95}$), but because there are already confusingly many quantities using the five percent cutoff, in this manuscript we focus on the 90% heterogeneity interval, and reserve 95% to describing uncertainties. 

To calculate a heterogeneity interval, we first specify the desired probability limits. For a 90% interval, we use the .05 and .95 percentiles (which define the central 90% of the distribution). Then, we pass those percentiles and the estimated mean and standard deviation (see Table \@ref(tab:tbl-fit-1)) to the normal quantile function ($q$; `qnorm()` in R) to obtain the interval: $HI_{90} = q([.05, .95], \beta_1, \tau_1) = q([.05, .95],$ `r mu1`, `r sd1`) = [`r hi90`]. In words, this equation calculates the 0.05 and 0.95 quantiles of the normal distribution defined by the mean's ($\beta_1$) and standard deviation's ($\tau_1$) point estimates: We expect 90% of valence effects in the population to fall within this interval. We illustrate this interval in Figure \@ref(fig:hdist-1)B.

### Proportion descriptors

For many applications, quantities such as proportions of slopes above or below some critical value, or within some critical range, might serve as more informative summaries. For example, we might ask "What proportion of individuals in the population respond faster to positively valenced words?" Thus, we first define a critical value, here zero. We then pass that value and the valence effect's estimated mean and standard deviation to the cumulative normal distribution function ($p$; `pnorm()` in R) to obtain the probability density in this distribution that falls below zero: $p^d = p(0, \beta_1, \tau_1) = p(0,$ `r mu1`, `r sd1`) = `r p_less_zero`. Specifically, this metric indicates the proportion of individuals in the population with negative effects. In words, we have estimated that `r p_less_zero` of the population respond faster to positively valenced words. We show this area under the curve in Figure \@ref(fig:hdist-1)C.

However, using zero as a critical value might not be sufficiently informative especially when theory allows defining a smallest effect sizes of interest, or what is known as a region of practical equivalence (to zero) [@anvariUsingAnchorbasedMethods2021; @kruschkeBayesianDataAnalysis2017; @kruschkeDoingBayesianData2014]. For example, we might know from theory that valence effects in the interval [-0.1, 0.1] are practically equivalent to zero. To calculate, we can again use the cumulative normal distribution function ($p$) to calculate the proportion of individuals in the population whose valence effect falls within this interval or region of practical equivalence: $p^{ROPE} = p(0.1, \beta_1, \tau_1) - p(-0.1, \beta_1, \tau_1)$ = `r p_rope`. In words, `r p_rope` of the population is expected to have valence effects that are practically equivalent to zero (within the [-0.1, 0.1] interval). We visualize this area under the curve in Figure \@ref(fig:hdist-1)D.

### Ratio descriptors

While the interval and ratio descriptors allow determining where the population's slope values are likely to fall, they do so in absolute terms, such as log(rt) in the running example. One way to contextualize the magnitude of heterogeneity is to put it in relative terms. To do so, we can compare the standard deviation of the valence effect distribution to its mean. [@bolgerCausalProcessesPsychology2019, p.609] suggest as a rule of thumb that if the ratio of the standard deviation to the average effect is 0.25 or greater, then heterogeneity can be deemed noteworthy. With these data and model, the ratio $\frac{\tau_1}{\beta_1}$ is `r ratio`, suggesting that the degree of heterogeneity in valence effects is noteworthy. (We reiterate the point made by @bolgerCausalProcessesPsychology2019 that the cutoff value of 0.25 is arbitrary and researchers should choose the cutoff based on their substantive goals.)

## Uncertainty

Although informative, the expected normal distribution of valence effects, or any transformation of those point estimates such as the ones illustrated in Figure \@ref(fig:hdist-1), ignores the uncertainty inherent in the estimated parameters, and therefore cannot communicate the analyst's uncertainty about the degree of heterogeneity. That is, the uncertainty with which we have estimated the population parameters, indicated by e.g. the confidence intervals in Table \@ref(tab:tbl-fit-1), is not carried forward to the calculations described above. For example, we only presented the expected heterogeneity interval, but not the range of credible values that it might take. For instance, if we re-calculate $HI_{90}$ based on the lowest 95%CI bounds in Table \@ref(tab:tbl-fit-1), the result is a heterogeneity interval that indicates 90% of the population's slopes falling in the [`r hi90_low`] interval. In contrast to the first interval, this interval implies much less heterogeneity and that 90% of the population's slopes are expected to fall below zero. 

We have now arrived at the crux of the current work: How should we estimate and describe heterogeneity in psychological phenomena such that the fundamental uncertainty in the estimated parameters is retained?

# Probabilistic assessment of heterogeneity

We think that probabilistic (i.e. "bayesian") methods are uniquely applicable to estimating and describing heterogeneity in psychological phenomena in ways that naturally incorporate information about uncertainty because they deliver distributions of plausible values that could underlie hypothetical data generating processes [@gelmanBayesianDataAnalysis2013; @kruschkeBayesianDataAnalysis2017; @kruschkeDoingBayesianData2014; @mcelreathStatisticalRethinkingBayesian2020]. Moreover, these methods are widely available in modern software packages; here we used the R package brms, which uses familiar R formulas for specifying bayesian models [@burknerAdvancedBayesianMultilevel2018; @burknerBrmsPackageBayesian2017; @rcoreteamLanguageEnvironmentStatistical2023]. 

```{r}
#| label: tbl-samples-1
#| cache: false

post1 <- as.data.frame(fit1, variable = c("b_", "sd_", "cor_", "sigma"), regex = TRUE) %>% 
  tibble() %>% 
  rownames_to_column("Sample")
post1_sum <- fit1 %>% 
  parameters(effects = "all", centrality = "mean", dispersion = TRUE) %>% 
  tibble() %>% 
  mutate(
    Mean = number(Mean, .01), 
    SD = number(SD, .001)
  ) %>% 
  select(Parameter, Mean, SD) %>% 
  pivot_longer(c(Mean, SD), names_to = "Sample") %>%
  pivot_wider(names_from = Parameter, values_from = value)

set.seed(191)
post1 %>% 
  slice_sample(n = 6) %>% 
  arrange(as.integer(Sample)) %>% 
  mutate(
    across(-Sample, ~number(., .01))
  ) %>%
  add_row(post1_sum) %>% 
  setNames(c("Sample", m1_parnames)) %>% 
  apa_table(
    span_text_columns = FALSE,
    font_size = "small",
    midrules = 6,
    caption = "Six random samples from Model 1's parameters' posterior distributions, and their summaries.", 
    escape = FALSE,
    placement = "h"
  )
```

The goal of bayesian inference is the multivariate posterior probability distribution of the model's parameters. However, because closed-form solutions are not available for posterior distributions of many important types of statistical models, in practice bayesian methods rely on drawing many random samples from the posterior distribution [@gelmanBayesianDataAnalysis2013; @ravenzwaaijSimpleIntroductionMarkov2016]. Therefore, "estimate" means a large number of random samples from the posterior distribution. These samples can then be used to calculate and summarize the posterior distribution of any desired quantity. 

In practice, one obtains (e.g.) 4,000 samples from the posterior distribution, and then summarizes them using familiar data processing techniques. To illustrate, Table \@ref(tab:tbl-samples-1) shows six random samples from the parameters' posterior distributions; the two bottom rows show their means and standard deviations. Note that because we used the brms package's default noninformative prior distributions, summaries of the resulting posterior distributions are numerically almost identical to the maximum likelihood estimates in Table \@ref(tab:tbl-fit-1). 

## Heterogeneity distribution

Armed with the bayesian estimates (Table \@ref(tab:tbl-samples-1)), we now return to the distribution of valence effects in the population. We first redraw the expected heterogeneity distribution from Figure \@ref(fig:hdist-1) using the posterior mean values of $\beta_1$ and $\tau_1$ in Figure \@ref(fig:hdist-2)A. Superimposed to that normal density curve are heterogeneity distributions calculated not from the posterior means, but from 100 random samples from the posterior distribution. From these curves we glean that the distribution of valence effects might well be narrower (i.e. less heterogeneity) or wider (more heterogeneity) than is suggested by the point estimates. 

Moreover, the location of these curves differ: Some curves are further to the left (average effect is more negative), and some further to the right (average effect is more positive). The distribution of these curves represents our uncertainty over the heterogeneity distribution of valence effects in the population. Thus, a good summary of heterogeneity that incorporates uncertainty must include information about uncertainty in both the scale (standard deviation) and location (mean) of the heterogeneity distribution.

```{r}
#| label: hdist-2
#| fig.height: 3.4
#| fig.width: 4
#| fig-cap: Posterior distribution of the heterogeneity distribution of valence effects (\textbf{A}) and its 90\% heterogeneity interval (\textbf{B}). \textbf{A}. Posterior mean heterogeneity distribution in dark blue, and 100 random draws from the distribution's posterior distribution in light blue. \textbf{B}. 100 random draws from the 90\% heterogeneity interval's posterior distribution.

set.seed(99)
x <- seq(-0.55, 0.25, length = 301)

calc_pdf <- function(posterior) {
  posterior %>% 
    crossing(x) %>% 
    mutate(
      pdf = dnorm(x, b_valence1, sd_person__valence1),
      cdf = pnorm(x, b_valence1, sd_person__valence1)
    )
}

dist_mean <- post1 %>% 
  mean_qi(b_valence1, sd_person__valence1) %>% 
  calc_pdf()

dist_samples <- post1 %>% 
  slice_sample(n = 100) %>% 
  calc_pdf()

p1 <- dist_mean %>% 
  ggplot(aes(x, pdf)) +
  scale_x_continuous(
    "Valence effect",
    expand = expansion(0),
    breaks = extended_breaks()
  ) +
  scale_y_continuous(
    "Density",
    expand = expansion(c(0.001, 0.05))
  ) +
  geom_line(
    linewidth = .6,
    col = "dodgerblue4"
  ) +
  geom_line(
    data = dist_samples,
    aes(group = Sample),
    col = "dodgerblue1",
    alpha = .33, 
    linewidth = .15
  ) +
  theme(
    axis.title.x = element_blank(), 
    plot.margin = unit(c(4,4,16,4), "pt")
  )
p2 <- distinct(dist_samples, Sample, b_valence1, sd_person__valence1) %>% 
  arrange(b_valence1) %>% 
  mutate(i = 1:n()) %>% 
  mutate(
    dist = dist_normal(b_valence1, sd_person__valence1)
  ) %>% 
  ggplot() +
  scale_x_continuous(
    "Valence effect",
    expand = expansion(0),
    breaks = extended_breaks()
  ) +
  aes(xdist = dist, y = i) +
  labs(y = "Sample") +
  stat_pointinterval(
    .width = .9,
    interval_size_range = c(.05, .1),
    alpha = .5,
    color = "dodgerblue1"
  )
(p1 / p2) &
  coord_cartesian(xlim = c(-0.55, 0.25)) &
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
  ) &
  plot_layout(heights = c(75, 25)) &
  plot_annotation(tag_levels = "A")
```

### Interval descriptors

To carry uncertainty forward from model parameters to their transformations, such as $HI_{90}$, we reproduce the calculations described above, but instead of using the point estimates we repeat the calculation over each posterior sample. In this manner, we obtain (samples from) posterior distributions of the parameter transformations, such as $HI_{90}$ (Figure \@ref(fig:hdist-2)B).

```{r}
p1s <- post1 %>% 
  mutate(
    hi_low = b_valence1 + qnorm(0.05) * sd_person__valence1,
    hi_high = b_valence1 + qnorm(0.95) * sd_person__valence1
  ) %>% 
  mean_qi(hi_low, hi_high) %>% 
  mutate(
    across(where(is.numeric), ~number(., .01)),
    l = str_glue("[{hi_low.lower}, {hi_low.upper}]"),
    u = str_glue("[{hi_high.lower}, {hi_high.upper}]")
  )
```

Summarising a distribution of intervals entails some challenges, however, because an interval is defined by two quantities---the lower and upper bounds. The 95% most credible lower bounds of $HI_{90}$ range between `r p1s$l`, whereas the 95% most credible upper bounds range between `r p1s$u`. Thus, to adequately describe an effects heterogeneity interval, researchers must communicate two separate credibility intervals. However, communicating these two credibility intervals is not only cumbersome, but also ignores potential correlations between the heterogeneity interval endpoints' posterior distributions. Because of these limitations, the heterogeneity interval may not always be the most desirable description of heterogeneity. 

### Proportion descriptors

```{r}
#| label: hdist-3
#| fig.height: 2.2
#| fig.width: 4
#| fig-cap: \textbf{A}. 100 random draws from the valence effect distribution's posterior distribution. Darker area under the curve represents the proportion of negative effects in the population for that posterior draw ($p^d$). \textbf{B}. Histogram of all 4,000 draws from the posterior distribution of $p^d$, and its posterior mean and 95%CI.

p1 <- distinct(dist_samples, Sample, b_valence1, sd_person__valence1) %>%
  arrange(b_valence1) %>%
  # slice(seq(1, nrow(.), by = 2)) %>%
  mutate(i = 1:n()) %>% 
  mutate(
    dist = dist_normal(b_valence1, sd_person__valence1)
  ) %>% 
  ggplot() +
  scale_fill_manual(values = c("dodgerblue1", "dodgerblue3"), na.value = "white") +
  scale_x_continuous(
    "Valence effect",
    expand = expansion(0),
    breaks = extended_breaks()
  ) +
  scale_y_continuous(
    "Sample",
    expand = expansion(0)
  ) +
  aes(xdist = dist, y = i) +
  geom_vline(xintercept = 0, linewidth = .2, linetype = "dashed") +
  stat_slab(
    aes(fill = after_stat(x < 0)), 
    height = 15,
    slab_color = "grey10",
    slab_linewidth = .1,
    alpha = .5,
    p_limits = c(.01, .99),
    show.legend = FALSE
  )
p2 <- post1 %>% 
  mutate(
    dist = dist_normal(b_valence1, sd_person__valence1),
    pd = cdf(dist, 0),
    prope = percent(cdf(dist, 0.1) - cdf(dist, -0.1), .1)
  ) %>% 
  ggplot(aes(pd)) +
  scale_y_continuous(
    "Count",
    expand = expansion(0)
  ) +
  scale_x_continuous(
    TeX('$p^d$'),
    breaks = extended_breaks(),
    labels = ~percent(., 1)
  ) +
  stat_histinterval(
    breaks = 50,
    outline_bars = TRUE,
    slab_color = "gray10",
    slab_fill = alpha("dodgerblue1", .33),
    slab_linewidth = .2,
    justification = .01,
    point_color = "dodgerblue4", 
    interval_color = "dodgerblue4",
    .width = .90
  )
(p1 | p2) &
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  ) &
  plot_annotation(tag_levels = "A")
```

```{r}
tmp <- post1 %>% 
  mutate(
    dist = dist_normal(b_valence1, sd_person__valence1),
    pd = cdf(dist, 0),
    rope = cdf(dist, 0.1) - cdf(dist, -0.1),
    ratio = abs(sd_person__valence1 / b_valence1)
  ) %>% 
  mean_qi(pd, rope, ratio) %>% 
  mutate(
    pd_r = str_glue(
      "{percent(pd, .1)} [{percent(pd.lower, .1)}, {percent(pd.upper, .1)}]"
    ),
    pd_r_n = str_glue(
      "{percent(1-pd, .1)} [{percent(1-pd.upper, .1)}, {percent(1-pd.lower, .1)}]"
    ),
    rope_r = str_glue(
      "{percent(rope, .1)} [{percent(rope.lower, .1)}, {percent(rope.upper, .1)}]"
    ),
    rope_r_n = str_glue(
      "{percent(1-rope, .1)} [{percent(1-rope.upper, .1)}, {percent(1-rope.lower, .1)}]"
    ),
    ratio_r = str_glue(
      "{number(ratio, .01)} [{number(ratio.lower, .01)}, {number(ratio.upper, .01)}]"
    )
  )
```

An alternative and often more useful description of heterogeneity is the proportion of the population whose effects fall above or below some critical value. In many applications, zero is a useful critical value because it allows us to calculate proportions of individuals with negative and positive effects. Figure \@ref(fig:hdist-3)A visualizes 100 random draws of the heterogeneity distribution, such that the area under each curve below zero is filled in darker grey. That area represents the proportion of individuals who respond faster to positive words. 

Therefore, we can ask "What proportion of individuals in the population have a negative effect of valence?" To answer, we calculate the probability density below zero of $\operatorname{Normal}(\beta_1, \tau_1)$ for each posterior sample of $\beta_1$ and $\tau_1$, and obtain samples of this density (Figure \@ref(fig:hdist-3)A). Figure \@ref(fig:hdist-3)B then shows these samples' histogram, and the posterior mean and 95% Credibility Interval (`r tmp$pd_r`). In words, we expect about `r percent(as.numeric(tmp$pd), .1)` to be negative, but with 95% credibility, this value could be as low as `r percent(as.numeric(tmp$pd.lower), .1)` or as high as `r percent(as.numeric(tmp$pd.upper), .1)`. Alternatively [@bolgerCausalProcessesPsychology2019, p.605-606], the model predicts that `r tmp$pd_r_n` of individuals in the population would show reversals of the valence effect.

However, a sign test against zero is not always the most informative quantity because it discards all information about the magnitude of effects. A more theoretically motivated approach is to argue for and define a smallest effect size of interest (SESOI) which then defines a region of practical equivalence (ROPE) to zero, and then quantify uncertainty for the proposition that the effect is "practically equivalent to zero". Figure \@ref(fig:hdist-4)A shows 100 random draws of the heterogeneity distribution of valence effects, such that the area under the curve within a ROPE of [-0.1, 0.1] ($p^{ROPE}$) is filled with a darker color. That shaded area represents the posterior probability, for each draw, that the effect is practically equivalent to zero. Figure \@ref(fig:hdist-4)B then shows a histogram of all 4,000 posterior draws of $p^{ROPE}$ with their mean and 95%CI. We learn that `r tmp$rope_r` of individuals in the population have a valence effect that is practically equivalent to zero.

```{r}
#| label: hdist-4
#| fig.height: 2.2
#| fig.width: 4
#| fig-cap: \textbf{A}. 100 random draws from the valence effect distribution's posterior distribution. Dark area under the curve represents the proportion of effects within a region of practical equivalence to zero (ROPE) in the population for that posterior draw ($p^{ROPE}$). \textbf{B}. Histogram of all 4,000 draws from the posterior distribution of $p^{ROPE}$, and its posterior mean and 95%CI.
p1 <- distinct(dist_samples, Sample, b_valence1, sd_person__valence1) %>% 
  arrange(b_valence1) %>% 
  # slice(seq(1, nrow(.), by = 2)) %>%
  mutate(i = 1:n()) %>% 
  mutate(
    dist = dist_normal(b_valence1, sd_person__valence1)
  ) %>% 
  ggplot() +
  scale_fill_manual(
    values = c("dodgerblue1", "dodgerblue3"), 
    na.value = "white"
    ) +
  scale_x_continuous(
    "Valence effect",
    expand = expansion(0),
    breaks = extended_breaks()
  ) +
  scale_y_continuous(
    "Sample",
    expand = expansion(0)
  ) +
  aes(xdist = dist, y = i) +
  geom_vline(xintercept = 0, linewidth = .2, linetype = "dashed") +
  stat_slab(
    aes(fill = after_stat(near(x, 0, .1))), 
    height = 15,
    slab_color = "grey10",
    slab_linewidth = .1,
    alpha = .5,
    p_limits = c(.01, .99),
    show.legend = FALSE
  )
p2 <- post1 %>% 
  mutate(
    dist = dist_normal(b_valence1, sd_person__valence1),
    pd = cdf(dist, 0),
    prope = cdf(dist, 0.1) - cdf(dist, -0.1)
  ) %>% 
  ggplot(aes(prope)) +
  scale_y_continuous(
    "Count",
    expand = expansion(0)
  ) +
  scale_x_continuous(
    TeX('$p^{ROPE}$'),
    breaks = extended_breaks(),
    labels = ~percent(., 1)
  ) +
  stat_histinterval(
    breaks = 50,
    outline_bars = TRUE,
    slab_color = "gray10",
    slab_fill = alpha("dodgerblue1", .33),
    slab_linewidth = .2,
    justification = .01,
    point_color = "dodgerblue4", 
    interval_color = "dodgerblue4",
    .width = .90
  )
(p1 | p2) &
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank()
  ) &
  plot_annotation(tag_levels = "A")
```

So far, these examples have highlighted the importance of quantifying uncertainty in measures of heterogeneity. Had we only focused on the point estimates (posterior means), we might have misleadingly concluded that $p^d$=`r percent(tmp$pd, .1)` and $p^{ROPE}$=`r percent(tmp$rope, .1)`. However, with 95% confidence, these values might be as great as `r percent(tmp$pd.upper, .1)` and `r percent(tmp$rope.upper, .1)`, respectively.

### Ratio descriptors

Finally, we can assess heterogeneity in relative terms by comparing the magnitude of the heterogeneity in valence effects (the standard deviation $\tau_1$) to the magnitude of the average effect (the mean $\beta_1$) by calculating the ratio $\frac{\tau_1}{\beta_1}$. Figure \@ref(fig:hdist-5)A shows 4,000 samples from the joint posterior distribution of the mean and standard deviation, which we then transformed into 4,000 samples of the posterior distribution of $\frac{\tau_1}{\beta_1}$ in Figure \@ref(fig:hdist-5)B. The ratio `r tmp$ratio_r` suggests that the relative magnitude of heterogeneity is substantial, but might be as low as `r number(tmp$ratio.lower, .01)`.

```{r}
#| label: hdist-5
#| fig.height: 2.2
#| fig.width: 4
#| fig-cap: \textbf{A}. 4,000 random draws from the posterior distribution of the valence effect distribution's mean ($\beta_1$) and standard deviation ($\tau_1$). \textbf{B}. Histogram of 4,000 draws from the posterior distribution of the ratio of the valence distribution's scale over its  location $\frac{\tau_1}{\beta_1}$, and its posterior mean and 95%CI.
p1 <- post1 %>% 
  ggplot(aes(b_valence1, sd_person__valence1)) +
  scale_y_continuous(
    TeX("$\\tau_1$")
  ) +
  scale_x_continuous(
    TeX("$\\beta_1$")
  ) +
  geom_point(
    shape = 1,
    col = "dodgerblue3",
    alpha = .25,
    size = .5
  )
p2 <- post1 %>% 
  mutate(ratio = abs(sd_person__valence1 / b_valence1)) %>% 
  ggplot(aes(ratio)) +
    scale_y_continuous(
    "Count",
    expand = expansion(0)
  ) +
  scale_x_continuous(
    TeX('$\\frac{\\tau_1}{\\beta_1}$'),
    breaks = extended_breaks()
  ) +
  stat_histinterval(
    breaks = 50,
    outline_bars = TRUE,
    slab_color = "gray10",
    slab_fill = alpha("dodgerblue1", .33),
    slab_linewidth = .2,
    justification = .01,
    point_color = "dodgerblue4", 
    interval_color = "dodgerblue4",
    .width = .90
  ) +
  theme(
    axis.ticks.y = element_blank(),
    axis.text.y = element_blank(),
  )
(p1 | p2) &
  theme(
    aspect.ratio = 1
  ) &
  plot_annotation(tag_levels = "A")
```

# Comparing heterogeneity between groups

We have now established methods for assessing and describing between-person heterogeneity in an experimental effect in a way that keeps track and describes uncertainty in this variability. In the following example, we take this idea further by addressing the following question: Does heterogeneity differ between groups? These groups could be e.g. different demographic groups that participated in an experiment ("Is the valence effect more variable among younger vs. older adults?") or different research groups in a multi-site project.

Accurately representing the uncertainty in model parameters is critical for addressing questions about differences: It is not sufficient to report that the standard deviation of valence effects was greater among one group compared to another. Instead, we need to quantify uncertainty in the differences, and the methods described above extend naturally to such questions.

As an example, we randomly divided the participants in the example data to two Groups A and B. These could represent, for example, two demographic groups such as younger and older participants. We then modified Model 1 to estimate separate population-level parameters ($\beta$, $\tau$, $\rho$) for the groups, allowing us to examine not only whether the average persons differ between the groups, but also the extent to which the two groups differ in heterogeneity.

```{r}
set.seed(999)
dat2 <- distinct(dat, person) %>% 
  mutate(
    g = sample(
      c("a", "b"), 
      length(unique(dat$person)), 
      replace = TRUE
    ),
    g = factor(g)
  ) %>% 
  right_join(dat)
contrasts(dat2$g) <- c(-0.5, 0.5)
```

```{r fit2-brms}
fit2 <- brm(
  bf(
    rt ~ 0 + g / valence + (1 + valence | gr(person, by = g)),
    sigma ~ 0 + g
  ),
  family = brmsfamily("gaussian", link = "identity", link_sigma = "identity"),
  data = dat2,
  file = "models/brm-fit-2",
  control = list(adapt_delta = .95)
)
```

Table \@ref(tab:tbl-fit-2) shows the model's estimated parameters and the heterogeneity metrics $p^d$, $p^{ROPE}$ (based on a ROPE of [-0.1, 0.1]), and $\frac{\tau_1}{\beta_1}$. Using these estimates (samples from each parameter's posterior distribution), we can extend the calculations from above to calculate differences between groups. Because we randomly divided participants into two groups, there are no broad differences between groups. 

```{r}
#| label: tbl-fit-2
#| cache: false

m2_parnames <- c(
  "$\\beta_0$", 
  "$\\beta_1$", 
  "$\\tau_0$", 
  "$\\tau_1$", 
  "$\\rho$",
  "$\\sigma$"
)
post2 <- gather_draws(fit2, `b_g.*`, `sd_.*`, `cor_.*`, `b_sigma.*`, regex = TRUE) %>% 
  ungroup() %>% 
  mutate(
    g = str_extract(.variable, "g[a|b]") %>% str_remove("g"),
    .variable = .variable %>% str_replace_all(":", "_") %>% str_remove_all("([_|:]g[a|b])+")
  )

post2 <- post2 %>% 
  pivot_wider(names_from = .variable, values_from = .value) %>% 
  mutate(
    dist = dist_normal(`b_valence1`, sd_person__valence1),
    pd = cdf(dist, 0),
    prope = cdf(dist, 0.1) - cdf(dist, -0.1),
    ratio = abs(sd_person__valence1 / `b_valence1`)
  ) %>% 
  select(-dist)

post2 %>% 
  pivot_longer(b:ratio) %>% 
  pivot_wider(names_from = g, values_from = value) %>% 
  rename(A = a, B = b) %>% 
  mutate(`B - A` = B - A) %>% 
  pivot_longer(c(A, B, `B - A`), names_to = "g") %>% 
  mutate(name = fct_inorder(name)) %>% 
  group_by(g, name) %>% 
  mean_qi(value) %>% 
  mutate(
    r = if_else(
      name %in% c("pd", "prope"),
      str_glue("{percent(value, .1)} [{number(.lower*100, .1)}, {number(.upper*100, .1)}]"),
      str_glue("{number(value, .01)} [{number(.lower, .01)}, {number(.upper, .01)}]")
      )
  ) %>% 
  mutate(
    name2 = rep(
      c(
        m2_parnames, 
        "$p^d$", 
        "$p^{ROPE}$", 
        "$\\frac{\\tau_1}{\\beta_1}$"
        ), 
      times = 3
    )
  ) %>% 
  select(Parameter = name2, g, r) %>% 
  mutate(g = str_glue("Group {toupper(g)}")) %>% 
  pivot_wider(names_from = g, values_from = r) %>% 
  apa_table(
    span_text_columns = FALSE,
    font_size = "scriptsize",
    caption = "Parameter estimates from Model 2.",
    escape = FALSE
  )
```


# Discussion

In the current work, we illustrated the use of practical descriptors of between-person heterogeneity in psychological phenomena. Moreover, we emphasized throughout that probabilistic modelling allows carrying uncertainty forward from model parameters to heterogeneity descriptors. The resulting metrics are useful because they not only convey the analyst's expectation of the magnitude of heterogeneity, but also her uncertainty in the description. In addition, probabilistic modelling, by returning a matrix of samples from the posterior distribution, allows easily comparing these heterogeneity descriptors across groups.

We believe that psychology should take a meta-theoretic step forward and try to incorporate an understanding of heterogeneity into substantive theories [@bolgerCausalProcessesPsychology2019]. In order to do so, descriptions of heterogeneity must include measures of uncertainty.

## Limitations

## Conclusion

# Competing interests

The author(s) declare no competing interests.

# Author contributions

Conceptualization: MV & NB    
Methodology: MV   
Software: MV    
Validation: MV    
Formal Analysis: MV   
Writing -- Original Draft: MV & NB    
Writing -- Review & Editing: MV & NB    
Visualization: MV & NB   
Project Administration: MV    

# References {-}

::: {#refs custom-style="Bibliography"}
:::
